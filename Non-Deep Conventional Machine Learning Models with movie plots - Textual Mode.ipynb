{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use two different machine learning models-\n",
    "1. SVM\n",
    "2. Naive Bayes\n",
    "\n",
    "Overview of steps coming forth\n",
    "<ul>\n",
    "<li>A little bit of feature engineering</li>\n",
    "<li>2 different Models\n",
    "<ul><li>SVM</li>\n",
    "<li>Naive Bayes</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Evaluation Metrics chosen\n",
    "<ul><li>precision: % of selected items that are correct = tp/(tp+fp) (tp: True Positive, fp: False Positive)</li>\n",
    "<li>Recall: % of correct items that are selected = tp/(tp+fn) (tp: True Positive, fn: False Negative)</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "<li>Model comparisons</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have improved the features by playing with the words in the overviews which represent textual data. For that I have used TF-IDf to assign weightage to every word in the bag of words. sklearn provides this facility via TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "import tmdbsimple as tmdb\n",
    "posterFolder='posters_final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1=open('X.pckl','rb')\n",
    "f2=open('Y.pckl','rb')\n",
    "X=pickle.load(f1)\n",
    "Y=pickle.load(f2)\n",
    "f1.close()\n",
    "f2.close()\n",
    "del f1\n",
    "del f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1559, 1344)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfTransformer = TfidfTransformer()\n",
    "XTfidf = tfidfTransformer.fit_transform(X)\n",
    "XTfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide X and Y into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(XTfidf.shape[0]) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ...,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open('movies_with_overviews.pckl','rb')\n",
    "moviesWithOverviews=pickle.load(f)\n",
    "f.close()\n",
    "del f\n",
    "\n",
    "f=open('Genredict.pckl','rb')\n",
    "GenreIDToName = pickle.load(f)\n",
    "f.close()\n",
    "del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XTrainTfidf=XTfidf[msk]\n",
    "XTestTfidf=XTfidf[~msk]\n",
    "YTrain=Y[msk]\n",
    "YTest=Y[~msk]\n",
    "positions=range(len(moviesWithOverviews))\n",
    "# print positions\n",
    "testMovies=np.asarray(positions)[~msk]\n",
    "#testMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: u'Adventure',\n",
       " 14: u'Fantasy',\n",
       " 16: u'Animation',\n",
       " 18: u'Drama',\n",
       " 27: u'Horror',\n",
       " 28: u'Action',\n",
       " 35: u'Comedy',\n",
       " 36: u'History',\n",
       " 37: u'Western',\n",
       " 53: u'Thriller',\n",
       " 80: u'Crime',\n",
       " 99: u'Documentary',\n",
       " 878: u'Science Fiction',\n",
       " 9648: u'Mystery',\n",
       " 10402: u'Music',\n",
       " 10749: u'Romance',\n",
       " 10751: u'Family',\n",
       " 10752: u'War',\n",
       " 10769: 'Foreign',\n",
       " 10770: u'TV Movie'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'kernel':['linear'], 'C':[0.01, 0.1, 1.0]}\n",
    "gridCV = GridSearchCV(SVC(class_weight='balanced'), parameters, scoring=make_scorer(f1_score, average='micro'))\n",
    "classif = OneVsRestClassifier(gridCV)\n",
    "\n",
    "classif.fit(XTrainTfidf, YTrain)\n",
    "GenreIDToName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genreNames=[u'Adventure',\n",
    "u'Fantasy',\n",
    "u'Animation',\n",
    "u'Drama',\n",
    "u'Horror',\n",
    "u'Action',\n",
    "u'Comedy',\n",
    "u'History',\n",
    "u'Western',\n",
    "u'Thriller',\n",
    "u'Crime',\n",
    "u'Documentary',\n",
    "u'Science Fiction',\n",
    "u'Mystery',\n",
    "u'Music',\n",
    "u'Romance',\n",
    "u'Family',\n",
    "u'War',\n",
    "'Foreign',\n",
    "u'TV Movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.40      0.52      0.45        60\n",
      "        Fantasy       0.43      0.51      0.47        51\n",
      "      Animation       0.42      0.54      0.47        41\n",
      "          Drama       0.58      0.62      0.60       124\n",
      "         Horror       0.42      0.48      0.44        42\n",
      "         Action       0.25      1.00      0.40        75\n",
      "         Comedy       0.00      0.00      0.00        70\n",
      "        History       0.09      1.00      0.17        28\n",
      "        Western       0.33      0.12      0.18         8\n",
      "       Thriller       0.44      0.63      0.52        82\n",
      "          Crime       0.38      0.64      0.47        33\n",
      "    Documentary       0.55      0.50      0.52        12\n",
      "Science Fiction       0.58      0.42      0.49        50\n",
      "        Mystery       0.26      0.28      0.27        32\n",
      "          Music       0.86      0.58      0.69        33\n",
      "        Romance       0.00      0.00      0.00        43\n",
      "         Family       0.56      0.55      0.56        58\n",
      "            War       0.78      0.39      0.52        18\n",
      "        Foreign       0.00      0.00      0.00         4\n",
      "       TV Movie       0.00      0.00      0.00        18\n",
      "\n",
      "    avg / total       0.39      0.51      0.41       882\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paritosh/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predstfidf=classif.predict(XTestTfidf)\n",
    "\n",
    "print classification_report(YTest, predstfidf, target_names=genreNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our model's predictions for a small sample of movies from our test set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genreList=sorted(list(GenreIDToName.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in range(XTestTfidf.shape[0]):\n",
    "    predGenres=[]\n",
    "    movieLabelScores=predstfidf[i]\n",
    "#     print movieLabelScores\n",
    "    for j in range(20):\n",
    "        #print j\n",
    "        if movieLabelScores[j]!=0:\n",
    "            genre=GenreIDToName[genreList[j]]\n",
    "            predGenres.append(genre)\n",
    "    predictions.append(predGenres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f=open('classifer_svc','wb')\n",
    "#pickle.dump(classif,f)\n",
    "#f.close()\n",
    "#del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVIE:  Justin and the Knights of Valour \tPREDICTION:  Action,History,Thriller\n",
      "MOVIE:  Igor \tPREDICTION:  Drama,Action,History,Thriller,Crime\n",
      "MOVIE:  Terminus \tPREDICTION:  Adventure,Fantasy,Action,History\n",
      "MOVIE:  Donkey X \tPREDICTION:  Action,History,Documentary,Music\n",
      "MOVIE:  Les Misérables \tPREDICTION:  Action,History,Thriller,Crime,Family\n"
     ]
    }
   ],
   "source": [
    "for i in range(XTestTfidf.shape[0]):\n",
    "    if i%60==0 and i!=0:\n",
    "        print 'MOVIE: ',moviesWithOverviews[i]['title'],'\\tPREDICTION: ',','.join(predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifnb = OneVsRestClassifier(MultinomialNB())\n",
    "classifnb.fit(X[msk].toarray(), YTrain)\n",
    "predsnb=classifnb.predict(X[~msk].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f=open('classifer_nb','wb')\n",
    "#pickle.dump(classifnb,f)\n",
    "#f.close()\n",
    "#del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsnb=[]\n",
    "for i in range(XTestTfidf.shape[0]):\n",
    "    predGenres=[]\n",
    "    movieLabelScores=predsnb[i]\n",
    "    for j in range(20):\n",
    "        #print j\n",
    "        if movieLabelScores[j]!=0:\n",
    "            genre=GenreIDToName[genreList[j]]\n",
    "            predGenres.append(genre)\n",
    "    predictionsnb.append(predGenres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "apiKey='b2db12a77bd83b7fa87202026d7f7ca5'\n",
    "tmdb.API_KEY = apiKey\n",
    "searchInstance = tmdb.Search()\n",
    "def get_poster(movie):\n",
    "    searchResponse = searchInstance.movie(query=movie)\n",
    "    id=searchResponse['results'][0]['id']\n",
    "    movie = tmdb.Movies(id)\n",
    "    posterPath=movie.info()['poster_path']\n",
    "    title=movie.info()['original_title']\n",
    "    url='image.tmdb.org/t/p/original'+posterPath\n",
    "    title='_'.join(title.split(' '))\n",
    "    strCmd='wget -O '+posterFolder+title+'.jpg '+url\n",
    "    os.system(strCmd)\n",
    "\n",
    "def get_movie_id(movie):\n",
    "    searchResponse = searchInstance.movie(query=movie)\n",
    "    movieId=searchResponse['results'][0]['id']\n",
    "    return movieId\n",
    "\n",
    "def get_movie_info(movie):\n",
    "    searchResponse = searchInstance.movie(query=movie)\n",
    "    id=searchResponse['results'][0]['id']\n",
    "    movie=tmdb.Movies(id)\n",
    "    info=movie.info()\n",
    "    return info\n",
    "\n",
    "def get_movie_genres(movie):\n",
    "    searchResponse = searchInstance.movie(query=movie)\n",
    "    id=searchResponse['results'][0]['id']\n",
    "    movie = tmdb.Movies(id)\n",
    "    genres=movie.info()['genres']\n",
    "    return genres\n",
    "\n",
    "def get_overview(movie):\n",
    "    searchResponse=searchInstance.movie(query=movie)\n",
    "    id=searchResponse['results'][0]['id']\n",
    "    movie=tmdb.Movies(id)\n",
    "    overview=movie.info()['overview']\n",
    "    return overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVIE:  Justin and the Knights of Valour PREDICTION:  Thriller ; Actual: [{u'id': 16, u'name': u'Animation'}, {u'id': 12, u'name': u'Adventure'}]\n",
      "MOVIE:  Igor PREDICTION:  Action,Thriller,Crime ; Actual: [{u'id': 16, u'name': u'Animation'}, {u'id': 35, u'name': u'Comedy'}, {u'id': 10751, u'name': u'Family'}, {u'id': 14, u'name': u'Fantasy'}]\n",
      "MOVIE:  Terminus PREDICTION:  Adventure,Fantasy,Animation,Comedy,Thriller,Family ; Actual: [{u'id': 878, u'name': u'Science Fiction'}, {u'id': 53, u'name': u'Thriller'}, {u'id': 14, u'name': u'Fantasy'}]\n",
      "MOVIE:  Donkey X PREDICTION:  Music ; Actual: [{u'id': 10751, u'name': u'Family'}, {u'id': 12, u'name': u'Adventure'}, {u'id': 16, u'name': u'Animation'}]\n",
      "MOVIE:  Les Misérables PREDICTION:  Animation,Thriller,Crime,Family ; Actual: [{u'id': 18, u'name': u'Drama'}, {u'id': 10402, u'name': u'Music'}, {u'id': 10749, u'name': u'Romance'}]\n"
     ]
    }
   ],
   "source": [
    "for i in range(XTestTfidf.shape[0]):\n",
    "    if i%60==0 and i!=0:\n",
    "        print 'MOVIE: ',moviesWithOverviews[i]['title'],'PREDICTION: ',','.join(predictionsnb[i]), ';','Actual:',get_movie_genres(moviesWithOverviews[i]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is some overlap between predicted and actual genre values. Now we need to check the model using Evaluation Metrics .\n",
    "### Here I am using Precision and recall metrics since this is a multi label problem, and conventionally precision and recall metrics are decent to be used when dealing with such problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall(gt,preds):\n",
    "    TP=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for t in gt:\n",
    "        if t in preds:\n",
    "            TP+=1\n",
    "        else:\n",
    "            FN+=1\n",
    "    for p in preds:\n",
    "        if p not in gt:\n",
    "            FP+=1\n",
    "    if TP+FP==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=TP/float(TP+FP)\n",
    "    if TP+FN==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=TP/float(TP+FN)\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.330065436771 0.531677704194\n"
     ]
    }
   ],
   "source": [
    "#Evaluation metric on Support Vector Machine\n",
    "precs=[]\n",
    "recs=[]\n",
    "for i in range(len(testMovies)):\n",
    "    if i%1==0:\n",
    "        pos=testMovies[i]\n",
    "        testMovie=moviesWithOverviews[pos]\n",
    "        gtids=testMovie['genre_ids']\n",
    "        gt=[]\n",
    "        for g in gtids:\n",
    "            gName=GenreIDToName[g]\n",
    "            gt.append(gName)\n",
    "#         print predictions[i],movies_with_overviews[i]['title'],gt\n",
    "        a,b=precision_recall(gt,predictions[i])\n",
    "        precs.append(a)\n",
    "        recs.append(b)\n",
    "\n",
    "print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48893866021 0.521633554084\n"
     ]
    }
   ],
   "source": [
    "#Evaluation metric on Naive Bayes Classifier\n",
    "precs=[]\n",
    "recs=[]\n",
    "for i in range(len(testMovies)):\n",
    "    if i%1==0:\n",
    "        pos=testMovies[i]\n",
    "        testMovie=moviesWithOverviews[pos]\n",
    "        gtids=testMovie['genre_ids']\n",
    "        gt=[]\n",
    "        for g in gtids:\n",
    "            gName=GenreIDToName[g]\n",
    "            gt.append(gName)\n",
    "#         print predictions[i],movies_with_overviews[i]['title'],gt\n",
    "        a,b=precision_recall(gt,predictionsnb[i])\n",
    "        precs.append(a)\n",
    "        recs.append(b)\n",
    "\n",
    "print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mean precision and recall scores for our test samples are pretty good! Models seem to be acceptable.\n",
    "\n",
    "Also, it can be infered that the Naive Bayes performs better than SVM in this scenario.\n",
    "And as it's well known that Multinomial Bayes is used for dealing with the problem of \"Document Classification\", Naive bayes also shows us better results because the model here is designed similar to the problem of document classification as each overview of a movie resembles a document and based on content of each overview, we classify the movie in different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
