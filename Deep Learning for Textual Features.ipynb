{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used- Word2Vec\n",
    "\n",
    "No need to forward propogate our data set through a network since the total number of words is small and,\n",
    "infact result is stored in the form of a dictionary, we need to simply look up the word in the dictionary and get the Word2Vec features for the word. \n",
    "\n",
    "The dictionary used is provided by googlenewsvector and it is available at: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a Python library designed to automatically extract semantic topics from documents.\n",
    "It can process raw, unstructured digital texts (“plain text”). The algorithms in gensim, such as Latent Semantic Analysis, Latent Dirichlet Allocation and Random Projections discover semantic structure of documents by examining statistical co-occurrence patterns of the words within a corpus of training documents. These algorithms are unsupervised, which means no human input is necessary - and we only need a corpus of plain text documents.\n",
    "\n",
    "Once these statistical patterns are found, any plain text documents can be succinctly expressed in the new, semantic representation and queried for topical similarity against other documents.\n",
    "\n",
    "The task of word2vec class(actually now a deprecated class and thus I have used KeyedVectors which is latest one) is training, using and evaluating unsupervised neural networks described in the paper: http://arxiv.org/pdf/1405.4053v2.pdf\n",
    "\n",
    "The algorithm represents each overview by a dense vector which is trained to predict words in the document.\n",
    "Its construction gives the algorithm potential to overcome the weaknesses of bag-of-words models. Empirical results shown in the paper prove that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations.\n",
    "Also, they chieve state-of-the-art results on several text classification and sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "modelDict = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, we can simply look up for a word in the above loaded model. \n",
    "#Like, to get the Word2Vec representation of the word \"movies\" we just do - modelDict['movies']\n",
    "modelDict['movies'].shape#We get output, a word2vector representation of movies.\n",
    "#And so, in this way, we can represent the words in our overview using word2Vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open('final_movies_set.pckl','rb')\n",
    "finalMoviesSet=pickle.load(f)\n",
    "f.close()\n",
    "del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1278"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalMoviesSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing we do here is - we delete commonly occurring words which we know are not informative about the genre. These words are often removed and are referred to as \"stop words\". Such words include simple words like \"a\", \"and\", \"but\", \"how\", \"or\" ,etc. They have been removed using the python package NLTK.\n",
    "\n",
    "Due to this kind of preprocessing, the net result is that movies with overviews which contain only stop words, or movies with overviews containing no words with word2vec representation are neglected.\n",
    "Others are used to build Mean word2vec representation. Concisely, preprocessing steps are as mentioned below:\n",
    "<ul>\n",
    "<li>Take movie overview</li>\n",
    "<li>Throw out stop words</li>\n",
    "<li>For non stop words:</li>\n",
    "</ul>\n",
    "If in word2vec - take it's word2vec representation which is 300 dimensional/\n",
    "If not - throw word\n",
    "<li>For each movie, calculate the arithmetic mean of the 300 dimensional vector representations for all words in the overview which weren't thrown out </li>\n",
    "\n",
    "This mean becomes the 300 dimensional representation for the movie. For all movies, these are stored in a numpy array. So the X matrix becomes (1278,300). And, Y is (1278,20) i.e. binarized 20 genres, as before\n",
    "\n",
    "I have taken arithmetic mean  and not kept all the words separately - because of limitation of how current neural networks work, the details related to which were taken from the paper: https://jiajunwu.com/papers/dmil_cvpr.pdf in context of multiple instance learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create list of english stop words\n",
    "enStop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1278, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "movieMeanWordvec=np.zeros((len(finalMoviesSet),300))\n",
    "movieMeanWordvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genres=[]\n",
    "rows_to_delete=[]\n",
    "for i in range(len(finalMoviesSet)):\n",
    "    mov=finalMoviesSet[i]\n",
    "    movie_genres=mov['genre_ids']\n",
    "    genres.append(movie_genres)\n",
    "    overview=mov['overview']\n",
    "    tokens = tokenizer.tokenize(overview)\n",
    "    stopped_tokens = [k for k in tokens if not k in enStop]\n",
    "    count_in_vocab=0\n",
    "    s=0\n",
    "    if len(stopped_tokens)==0:\n",
    "        rows_to_delete.append(i)\n",
    "        genres.pop(-1)\n",
    "#         print overview\n",
    "#         print \"sample \",i,\"had no nonstops\"\n",
    "    else:\n",
    "        for tok in stopped_tokens:\n",
    "            if tok.lower() in modelDict.vocab:\n",
    "                count_in_vocab+=1\n",
    "                s+=modelDict[tok.lower()]\n",
    "        if count_in_vocab!=0:\n",
    "            movieMeanWordvec[i]=s/float(count_in_vocab)\n",
    "        else:\n",
    "            rows_to_delete.append(i)\n",
    "            genres.pop(-1)\n",
    "#             print overview\n",
    "#             print \"sample \",i,\"had no word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1261"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask2=[]\n",
    "for row in range(len(movieMeanWordvec)):\n",
    "    if row in rows_to_delete:\n",
    "        mask2.append(False)\n",
    "    else:\n",
    "        mask2.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1261, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=movieMeanWordvec[mask2]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1261, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb=MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(genres)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textualFeatures=(X,Y)\n",
    "#f=open('textual_features.pckl','wb')\n",
    "#pickle.dump(textual_features,f)\n",
    "#f.close()\n",
    "#del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f=open('textual_features.pckl','rb')\n",
    "#textualFeatures=pickle.load(f)\n",
    "#f.close()\n",
    "#del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1261, 300)\n",
      "(1261, 20)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maskText=np.random.rand(len(X))<0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XTrain=X[maskText]\n",
    "YTrain=Y[maskText]\n",
    "XTest=X[~maskText]\n",
    "YTest=Y[~maskText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training steps are similar to the case of deep learning in case of Posters\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "modelTextual = Sequential([\n",
    "    Dense(300, input_shape=(300,)),\n",
    "    Activation('relu'),\n",
    "    Dense(20),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "modelTextual.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "990/990 [==============================] - 0s - loss: 0.4237 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "990/990 [==============================] - 0s - loss: 0.4203 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "990/990 [==============================] - 0s - loss: 0.4174 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "990/990 [==============================] - 0s - loss: 0.4141 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "990/990 [==============================] - 0s - loss: 0.4109 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "990/990 [==============================] - 0s - loss: 0.4080 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "990/990 [==============================] - 0s - loss: 0.4052 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "990/990 [==============================] - 0s - loss: 0.4026 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "990/990 [==============================] - 0s - loss: 0.4003 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "990/990 [==============================] - 0s - loss: 0.3974 - acc: 0.8497     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f34ee794b10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTextual.fit(XTrain, YTrain, epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f34e5ff6f90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTextual.fit(XTrain, YTrain, epochs=10000, batch_size=500,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "249/271 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = modelTextual.evaluate(XTest, YTest, batch_size=249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 85.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (modelTextual.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 27,\n",
       " 28,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 53,\n",
       " 80,\n",
       " 99,\n",
       " 878,\n",
       " 9648,\n",
       " 10402,\n",
       " 10749,\n",
       " 10751,\n",
       " 10752,\n",
       " 10769,\n",
       " 10770]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YPreds=modelTextual.predict(XTest)\n",
    "#print YPreds\n",
    "\n",
    "f=open('Genredict.pckl','rb')\n",
    "GenreIDToName=pickle.load(f)\n",
    "f.close()\n",
    "del f\n",
    "GenreIDToName\n",
    "genreList=sorted(GenreIDToName.keys())\n",
    "genreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall(gt,preds):\n",
    "    TP=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for t in gt:\n",
    "        if t in preds:\n",
    "            TP+=1\n",
    "        else:\n",
    "            FN+=1\n",
    "    for p in preds:\n",
    "        if p not in gt:\n",
    "            FP+=1\n",
    "    if TP+FP==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=TP/float(TP+FP)\n",
    "    if TP+FN==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=TP/float(TP+FN)\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our predictions for the movies are - \n",
      "\n",
      "Predicted:  [u'Adventure', u'Thriller', u'Action']  Actual:  [u'Adventure', u'Action', u'Comedy', u'Romance']\n",
      "Predicted:  [u'Comedy', u'Thriller', u'Drama']  Actual:  [u'Drama', u'Comedy', u'Romance']\n",
      "Predicted:  [u'Comedy', u'Thriller', u'Drama']  Actual:  [u'Drama', u'Comedy', u'Thriller', u'Crime']\n",
      "Predicted:  [u'Action', u'Thriller', u'Drama']  Actual:  [u'Drama', u'Thriller', u'Crime', u'Romance']\n",
      "Predicted:  [u'Drama', u'Family', u'Comedy']  Actual:  [u'Animation', u'Music', u'Family']\n",
      "Predicted:  [u'Drama', u'Family', u'Adventure']  Actual:  [u'Adventure', u'Animation', u'Comedy', u'Science Fiction', u'Family']\n"
     ]
    }
   ],
   "source": [
    "print \"Our predictions for the movies are - \\n\"\n",
    "precs=[]\n",
    "recs=[]\n",
    "for i in range(len(YPreds)):\n",
    "    row=YPreds[i]\n",
    "    gtGenres=YTest[i]\n",
    "    gtGenreNames=[]\n",
    "    for j in range(20):\n",
    "        if gtGenres[j]==1:\n",
    "            gtGenreNames.append(GenreIDToName[genreList[j]])\n",
    "    top3=np.argsort(row)[-3:]\n",
    "    predictedGenres=[]\n",
    "    for genre in top3:\n",
    "        predictedGenres.append(GenreIDToName[genreList[genre]])\n",
    "    (precision,recall)=precision_recall(gtGenreNames,predictedGenres)\n",
    "    precs.append(precision)\n",
    "    recs.append(recall)\n",
    "    if i%50==0:\n",
    "        print \"Predicted: \",predictedGenres,\" Actual: \",gtGenreNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509713261649 0.563918757467\n"
     ]
    }
   ],
   "source": [
    "print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))\n",
    "#print 0.509713261649, 0.563918757467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
